[{"title":"使用谷歌reCAPTCHA的v2和v3对网站应用进行“图像验证码/行为评分”反爬","date":"2020-12-31T13:21:00.000Z","url":"/2020/12/31/%E4%BD%BF%E7%94%A8%E8%B0%B7%E6%AD%8CreCAPTCHA%E7%9A%84v2%E5%92%8Cv3%E5%AF%B9%E7%BD%91%E7%AB%99%E5%BA%94%E7%94%A8%E8%BF%9B%E8%A1%8C%E2%80%9C%E5%9B%BE%E5%83%8F%E9%AA%8C%E8%AF%81%E7%A0%81-%E8%A1%8C%E4%B8%BA%E8%AF%84%E5%88%86%E2%80%9D%E5%8F%8D%E7%88%AC/","tags":[["python","/tags/python/"],["反爬","/tags/%E5%8F%8D%E7%88%AC/"],["验证码","/tags/%E9%AA%8C%E8%AF%81%E7%A0%81/"]],"categories":[["反爬","/categories/%E5%8F%8D%E7%88%AC/"],["python","/categories/python/"],["验证码","/categories/%E9%AA%8C%E8%AF%81%E7%A0%81/"]],"content":"0.了解谷歌reCAPTCHAreCAPTCHA是谷歌推出的免费人机识别技术，使用图像验证码&#x2F;用户行为评分来区别当前网站应用的用户是机器人还是正常人类。当前有v2版本和新的v3版本，当然，攻守永恒，破解reCAPTCHA的大有人在，unCaptcha2就是一个例子，相关： reCAPTCHA官网-（需要科学上网） reCAPTCHA代码仓库-Github reCAPTCHA验证流程在线demo reCAPTCHA破解库unCaptcha2代码仓库Github 作为谷歌强推的一款人机识别利器，reCAPTCHA v2使用图像&#x2F;语音识别技术来区分人类和机器;reCAPTCHA v3则通过分析网站应用的大量用户流量进而使用其机器学习技术来识别用户行为并为其打分，分值范围0.0~1.0,得分越低的越表明当前用户是机器人。 假设我们的网站应用是属于登录/注册后进而后续操作的类型，那么防止用户滥注册或机器人登录破坏网站应用生态尤为重要，反爬&#x2F;anti-robot迫在眉睫.而第一关卡就是阻止机器人&#x2F;恶意注册程序进入我们网站应用的数据空间，将其拒之门外或者诱敌深入（爬虫蜜罐）。谷歌reCAPTCHA或许可以帮助我们实现基础的第一步：拒机器人&#x2F;恶意程序于门外。 以下简称恶意程序或机器人为攻击者. 1.验证码(CAPTCHA)一分为二地看，我们的网站应用被恶意爬取或机器人造访，必是有可图之物，比如某个功能强大的路由（页面工具），对于攻击者的意图有针对性的进行防护是基础反爬意识；其次，基于“料敌从宽 御敌从严”的理念，我们有必要预设攻击者的攻击模型，即攻击者可能使用的突破反爬的技术手段，进而采取对应的反爬措施。 对于我们的网站应用，有价值的工具或者页面操作需要在注册/登录后才能进一步访问，因而阻止攻击者高频次的注册&#x2F;登录是首要矛盾。注册&#x2F;登录验证码可以有效过滤掉一波攻击者。市面上的验证码一般有： 以上种种验证码只是当前验证码市场的一小部分，针对图像验证码，可以根据机器学习技术对大量的训练集进行学习，进一步达到破解的目的，亦或是接入人工打码平台，而对于简单的字符数字识别验证码，则有对应的成熟的第三方库可以轻易破解，网站应用加入验证码的目的只是提高或者延长攻击者的破解周期和成本代价而已。 2.Why reCAPTCHA?谷歌reCAPTCHA两个版本的数据流均是前端点击识别&#x2F;用户环境生成关键参数g-recaptcha-response对服务端进行验证查询,类似如下： 此值唯一且无法重复使用，通过用户环境生成，可以作为注册登录携带的必要参数，服务端通过私钥与谷歌通信查询验证此值有效性从而达到识别人机的作用。 3.使用前准备3.1 注册recaptcha密钥对登录Google reCAPTCHA（需科学上网）管理界面，点击创建recaptcha密钥对： 3.2 保存密钥对 第一个密钥为内嵌在网站应用上的site_key,第二个为服务端与Google通信用的密钥secret_key，不能外泄。 4.接入reCAPTCHA本文使用flask作为后台服务框架，demo环境： python3.7 win10 64位 requests 文件结构： 密钥对配置： 4.1 recaptcha v2 复选框checkbox 模式 前端页面recaptcha-v2-checkbox.html： 后端路由及验证函数： 其中装饰器verify_recaptcha是两个版本统一与Google recaptcha通信验证的函数: 验证通过后返回数据示例： 验证失败返回示例： 其中，success字段为true则表明验证通过，为false则表明验证失败。 对于我们想要使用recaptcha v2进行防护的任意路由，在其页面引入v2版本的html元素后，我们在服务端可以直接安上装饰器verify_recaptcha,例如对登录路由进行保护: 在线测试demo: 点击&#x3D;&#x3D;&gt;recaptcha.1991.site 4.2 recaptcha v3 默认自动绑定渲染模式 前端页面recaptcha-v3.html： 后端路由及验证函数： 验证通过后返回数据示例： v3版本的体验比v2版本的更加顺畅，因为其依赖的是评分机制，越接近0.0分则越表明当前用户是机器人从而可以给服务端进行对应处理。但是前提是当前网站的流量必须积累一段时间评分才会愈加精确，有更多的用户流量行为数据进行训练才会更加完善。如果担心用户隐私的问题，建议慎重选择 在线测试demo: 点击&#x3D;&#x3D;&gt;recaptcha.1991.site v2\\v3其他模式的完整代码示例可见GitHub仓库:recaptcha-flask-demo 5.后台流量查看登录Google reCAPTCHA admin后台面板可查看最近网站应用recaptcha的流量情况:"},{"title":"得物PC微信小程序接口解析","date":"2020-11-23T04:27:00.000Z","url":"/2020/11/23/%E5%BE%97%E7%89%A9PC%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F%E6%8E%A5%E5%8F%A3%E8%A7%A3%E6%9E%90/","tags":[["python","/tags/python/"],["小程序","/tags/%E5%B0%8F%E7%A8%8B%E5%BA%8F/"],["破解","/tags/%E7%A0%B4%E8%A7%A3/"],["逆向","/tags/%E9%80%86%E5%90%91/"]],"categories":[["python","/categories/python/"],["破解","/categories/%E7%A0%B4%E8%A7%A3/"]],"content":"抓包 Fiddler windows 10 64bit 页面对应接口1.商品详情页面截图：抓包截图： 对应的接口为： 请求头部： 请求参数(json格式)： 可见其中的sign为加密参数，利用上一篇《**反编译PC微信小程序**》的小记,可拿到加密代码，翻译为python为： 伪造请求： 返回结果： "},{"title":"反编译PC微信小程序","date":"2020-11-20T08:50:06.000Z","url":"/2020/11/20/%E5%8F%8D%E7%BC%96%E8%AF%91PC%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F/","tags":[["python","/tags/python/"],["小程序","/tags/%E5%B0%8F%E7%A8%8B%E5%BA%8F/"],["破解","/tags/%E7%A0%B4%E8%A7%A3/"],["逆向","/tags/%E9%80%86%E5%90%91/"]],"categories":[["python","/categories/python/"],["破解","/categories/%E7%A0%B4%E8%A7%A3/"]],"content":"概述微信是移动社交的巨无霸，任何移动端版本的wechat，想要逆向都是很困难的。但是企鹅意欲打造的社交生态圈覆盖PC和移动端，移动端无法解决的事情，可以从其他的途径找到某些眉目。小程序是新的一个突破口，微信小程序已经成为很多移动端app的流量引入平台，那么被数据采集也无可避免。 环境 示例微信小程序：今日最热榜 平台：Windows 10 64bit Python 3.7 64bit NodeJs v12.16.1 工具 微信小程序包反编译工具wxappUnpacker 微信pc小程序解密脚本 抓包工具Fiddler 0.解密小程序wxapkg包pc端的微信小程序与移动端不同，pc端对小程序包(例如__APP__.wxapkg)进行了加密,加密详细请见此处，所以我们使用微信小程序反编译工具来反编译wxapkg包会报错，一般存放某个pc微信小程序包的路径为： 类似下图： 其中的wx5bfb6b535af31da3便是微信小程序今日最热榜的wxid,此字符串用于对小程序包的解密，需要用到。 ps:如果发现找不到对应的wxapkg包，那么打开PC微信，并打开小程序今日最热榜便可以将其小程序包下载至本地，如果在一下路径中发现包太多，那么全部删除剩下publicLib文件夹，再打开今日最热榜便可以看到唯一一个小程序包文件夹。 此时解密小程序包的条件已达成： 小程序包APP.wxapkg 小程序wxid:wx5bfb6b535af31da3 安装Crypto的python3.7环境 解密python脚本 解密脚本decrypt.py： 开始解密小程序包: 本例使用命令(小程序包__APP__.wxapkg与解密脚本放在同一路径下)： 此时成功会发现当前路径下有新包：decryptedAPP.wxapkg，便是解密后的小程序包 类似下图： 1.反编译wxapkg包获取到解密后的wxapkg，那么可以使用wxappUnpacker对解密后的decryptedAPP.wxapkg进行反编译，首先安装wxappUnpacker,详见此处,安装后将decryptedAPP.wxapkg放在wxappUnpacker文件夹下，反编译命令： 本例使用命令： 类似下图： 成功后会发现与decryptedAPP.wxapkg同个路径下有同名文件夹decryptedAPP，结构类似： 至此简单的反编译流程跑通。 2.寻找关键加密请求字段本例中的小程序今日最热榜，打开后使用Fiddler进行抓包分析其接口参数： 下拉小程序加载新闻流，可以发现对应接口为： 其中请求头部： 对应参数：其中加密参数为：as,cp 使用WebStorm打开文件夹decryptedAPP，全局查找关键词as:（注意有加英文冒号）,结果如下：js文件ASCP.js内容： 可以知道加密参数as,cp的生成代码，直接翻译为python: 那么请求接口便可以进行完整的构造。 3.构造请求根据抓包获取到的接口路径和请求头部，利用加密参数构造请求： 返回结果： "},{"title":"关于Python爬虫种类、法律、轮子的一二三","date":"2020-11-16T05:57:12.000Z","url":"/2020/11/16/%E5%85%B3%E4%BA%8EPython%E7%88%AC%E8%99%AB%E7%A7%8D%E7%B1%BB%E3%80%81%E6%B3%95%E5%BE%8B%E3%80%81%E8%BD%AE%E5%AD%90%E7%9A%84%E4%B8%80%E4%BA%8C%E4%B8%89/","tags":[["爬虫","/tags/%E7%88%AC%E8%99%AB/"],["法律","/tags/%E6%B3%95%E5%BE%8B/"]],"categories":[["python","/categories/python/"],["爬虫","/categories/%E7%88%AC%E8%99%AB/"]],"content":"Welcome to the D-age对于网络上的公开数据，理论上只要由服务端发送到前端都可以由爬虫获取到。但是Data-age时代的到来，数据是新的黄金，毫不夸张的说，数据是未来的一切。基于统计学数学模型的各种人工智能的出现，离不开数据驱动。数据采集、清洗是最末端的技术成本，网络爬虫也是基础采集脚本。但是有几个值得关注的是： 对于实时变化的网络环境，爬虫的持续有效性如何保证 数据采集、清洗规则的适用范围 数据采集的时间与质量–效率 爬与反爬的恩怨 爬虫的法律界限 法律的边界,技术无罪对于上面几个关注点，我最先关注的便是爬虫的法律界限 ，我曾经咨询过一个律师： Q: 老师，我如果用爬虫爬取今日头条这种类型网站的千万级公开数据，算不算违法呢？A: 爬取的公开数据不得进行非法使用或者商业利用 简单的概括便是爬虫爬取的数据如果进行商业出售或者有获利的使用，便构成了“非法使用”。而一般的爬虫程序并不违法，其实这是从法律专业的一方来解读，如果加上技术层面的维度，那么应该从这几方面考虑： 爬取的数据量 爬取数据的类型（数据具有巨大的商业价值，未经对方许可，任何人不得非法获取其数据并用于经营行为） 爬取的数据用途 (同行竞争？出售？经营？分析？实验？…) 是否遵循网站的robots.txt 即 机器人协议 爬取行为是否会对对方网站造成不能承受的损失（大量的爬取请求会把一个小型网站拖垮） 其实爬虫构成犯罪的案例是开始增多的，相关新闻: 当爬虫遇上法律会有什么风险？ 程序员爬虫竟构成犯罪？ 爬虫相关法律知识 如果你的上级或公司要求你爬取某些网站的大量公开数据，你会怎么办呢？可以参考第2条新闻。法律矛盾点关键在于前面考虑的前三点，如果是个人隐私数据，是不能爬取的，如果是非公开数据，是不能爬取的，而对于其他大量的公开数据爬取，看人家查不查的到你，要不要起诉你。技术在你的手上，非法与否在于你怎么去用。最好的爬取道德原则是： 减少并发请求 延长请求间隔 不进行公开出售数据 遵循网站 robots协议 当然，反爬最有效的便(目的均在于拦截爬虫进入网站数据范围)是: 要求用户密码+验证码 加密数据 js混淆 css混淆 针对IP请求频率封锁 针对cookie、session单个账户请求频率封锁单日请求次数 对关键数据进行拆分合并 对爬虫投毒（返回假数据） 完善robots.txt 识别点击九宫图中没有包含xxx的图片等（终极验证码) 设置黑白名单、IP用户组等 工欲善其事针对网站的公开数据进行爬取，我们一般都要先对网站数据进行分析，定位，以确定其采集规则，如果网站设置了访问权限，那么便不属于我们的爬虫采集范围了:)分析好采集规则，写好了采集数据持久化（存入数据库、导出为word、excel、csv、下载等）的相关代码，整个爬虫运行正常。那么怎样才能提高采集速度呢？ 多进程采集 多线程采集 异步协程采集 多进程 + 多线程采集 多进程 + 异步协程采集 分布式采集 异步爬虫是同步爬虫的升级版，在同步爬虫中，无论你怎么优化代码，同步IO的阻塞是最大的致命伤。同步阻塞会让采集任务一个个排着长队领票等待执行。而异步采集不会造成IO阻塞，充分利用了IO阻塞任务的等待时间去执行其他任务。 在IO 模型中，只有IO多路复用（I&#x2F;O multiplexing）{在内核处理IO请求结果为可读或可写时调用回调函数} 不阻塞 “内核拷贝IO请求数据到用户空间”这个过程，实现异步IO操作。 同步爬虫一般的同步爬虫，我们可以写一个，（以爬取图片网站图片为例），我们来看看其下载该网址所有图片所花费的时间： 以下代码为后面多个例程的共同代码: 同步爬虫: 执行同步爬虫， 输出（时间可能不一样，取决于你的网速）： 在同一个网络环境下，排除网速时好时坏，可以下载多几次取平均下载时间，在我的网络环境下，我下载了5次，平均耗时约55.26s 多进程爬虫所以为了提高采集速度，我们可以写一个多进程爬虫（以爬取图片网站图片为例）:为了对应多进程的进程数n，我们可以将图片链接列表分成n组，多进程爬虫: 执行爬虫,进程数设为4，一般是cpu数量： 输出: 可以看出，多进程比原先的同步爬虫快许多，整个程序耗时19.51s，为什么不是同步爬虫的55s&#x2F;4 ≈ 14s呢？因为进程间的切换需要耗时。如果把进程数增大，那么: 对于多进程爬虫来说，虽然实现异步爬取，但也不是越多进程越好，进程间切换的开销不仅会让你崩溃，有时还会让你的程序崩溃。一般用进程池Pool维护，Pool的processors设为CPU数量。进程的数量设置超过100个便让我的程序崩溃退出。使用进程池可以保证当前在跑的进程数量控制为设置的数量，只有池子没满才能加新的进程进去。 多线程爬虫多线程版本可以在单进程下进行异步采集，但线程间的切换开销也会随着线程数的增大而增大。当线程间需要共享变量内存时，此时会有许多不可预知的变量读写操作发生，python为了使线程同步，给每个线程共享变量加了全局解释器锁GIL。而我们的爬虫不需要共享变量，因此是线程安全的，不用加锁。多线程版本： 并发线程数太多会让我们的系统开销越大，使程序花费时间越长，同时也会增大目标网站识别爬虫机器行为的几率。因此设置好一个适当的线程数以及爬取间隔是良好的爬虫习惯。执行多线程爬虫，设置线程数为50 输出： 增大线程数，输出： 可以看到，线程可以有效的提高爬取效率，缩短爬取时间，但必须是一个合理的线程数，越多有时并不是越好的，一般是几十到几百个之间，数值比多进程进程数大许多。 异步协程爬虫Python3.5引入了async&#x2F;await 异步协程语法。详见PEP492由于asyncio提供了基于socket的异步I&#x2F;O，支持TCP和UDP协议，但是不支持应用层协议HTTP，所以需要安装异步http请求的aiohttp模块单进程下的异步协程爬虫： 执行异步协程爬虫，设置最大并发请求数为100： 输出： 可以看出，异步多协程的下载请求效率并不比多线程差，由于磁盘IO读写阻塞，所以还可以进一步优化，使用aiofiles。针对比较大的多媒体数据下载，异步磁盘IO可以使用aiofiles,以上述例子download可以改为: 多进程 + 多线程 爬虫实际采集大量数据的过程中，往往是多种手段来实现爬虫，这样可以充分利用机器CPU，节省采集时间。下面使用多进程（进程数为CPU数，4）+ 多线程 （线程数设为50）来对例子进行更改(上面各个例子导入的模块默认使用): 执行爬虫： 输出： 采集时间与异步协程和多线程并无多大的差异，可以使用更大数据量做实验区分。因为多进程+多线程，CPU切换上下文也会造成一定的开销，所以进程数与线程数不能太大，并发请求的时间间隔也要考虑进去。 多进程 + 异步协程 爬虫使用多进程（进程数为CPU数，4）+ 异步协程（最大并发请求数设为50）来对例子进行更改(上面各个例子导入的模块默认使用): 执行爬虫 ： 输出： 效果与多进程 + 多线程 爬虫差不多，但是CPU减少了切换线程上下文的开销，而是对每一个协程任务进行监视回调唤醒。使用IO多路复用的底层原理实现。 分布式采集关于分布式采集将会单独写一章，使用Map-Reduce+redis来实现分布式爬虫。 轮子们，你们辛苦了现实生活中的爬虫不止上面那些，但是基本的骨架是一样的，对于特定的网站需要制定特定的采集规则，所以通用的数据采集爬虫很难实现。所以针对某个网站的数据采集爬虫是需要定制的，但是在不同之中包含着许多的相同、重复性的过程，比如说采集流程，或者对请求头部的伪造，数据持久化的处理等，采集框架应运而生。Scrapy就是目前比较成熟的一个爬虫框架。它可以帮助我们大大减少重复性的代码编写，可以更好的组织采集流程。而我们只需要喝一杯咖啡，编写自己的采集规则，让Scrapy去给我们管理各种各样的爬虫，做些累活。如果你是一个爬虫爱好者，那么scrapy是你的不错选择。由于好奇scrapy的实现流程，所以我才开始打开他的源码学习。有些人觉得scrapy太重，他的爬虫只需要简单的采集，自己写一下就可以搞定了。但如果是大量的爬虫采集呢？怎么去管理这些爬虫呢？怎样才能提高采集效率呀？Scrapy helps！！ 另外还有另一个Python采集框架：pyspider。国人编写的，cool感谢轮子们的父母，还有那些辛苦工作的轮子们，你们辛苦了"}]